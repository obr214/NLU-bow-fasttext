{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "def get_reviews(path, clean = True):\n",
    "    complete_path = path + '/*.txt'\n",
    "    files = glob.glob(complete_path)    \n",
    "    reviews = [str(open(rev).readlines()[0]).strip() for rev in files]\n",
    "    # Removes the tag <br />\n",
    "    reviews = [rev.replace('<br />',' ') for rev in reviews]\n",
    "    if clean:\n",
    "        reviews = [clean_str(rev) for rev in reviews]\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Gets all the reviews\n",
    "train_positive_reviews = get_reviews(\"data/aclImdb/train/pos\")\n",
    "train_negative_reviews = get_reviews(\"data/aclImdb/train/neg\")\n",
    "test_positive_reviews = get_reviews(\"data/aclImdb/test/pos\")\n",
    "test_negative_reviews = get_reviews(\"data/aclImdb/test/neg\")\n",
    "\n",
    "# Divide The train set into train and validation\n",
    "\n",
    "# Concat all train reviews and write it on a file\n",
    "train_reviews = train_positive_reviews + train_negative_reviews\n",
    "output_train = open('data/all_train.txt', 'w')\n",
    "for rev in train_reviews:\n",
    "    print>>output_train, rev\n",
    "output_train.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saves the Train/Test lists into pickle objects\n",
    "pickle.dump(train_positive_reviews, open( \"data/train_pos.p\", \"wb\" ))\n",
    "pickle.dump(train_negative_reviews, open( \"data/train_neg.p\", \"wb\" ))\n",
    "pickle.dump(test_positive_reviews, open( \"data/test_pos.p\", \"wb\" ))\n",
    "pickle.dump(test_negative_reviews, open( \"data/test_neg.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loads the Train/Test objects\n",
    "train_positive_reviews = pickle.load(open(\"data/train_pos.p\",\"rb\"))\n",
    "train_negative_reviews = pickle.load(open(\"data/train_neg.p\",\"rb\"))\n",
    "test_positive_reviews = pickle.load(open(\"data/test_pos.p\",\"rb\"))\n",
    "test_negative_reviews = pickle.load(open(\"data/test_neg.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_train_sets():\n",
    "    train_positive_reviews = pickle.load(open(\"data/train_pos.p\",\"rb\"))\n",
    "    train_negative_reviews = pickle.load(open(\"data/train_neg.p\",\"rb\"))\n",
    "    return train_positive_reviews, train_negative_reviews\n",
    "\n",
    "def get_test_sets():\n",
    "    test_positive_reviews = pickle.load(open(\"data/test_pos.p\",\"rb\"))\n",
    "    test_negative_reviews = pickle.load(open(\"data/test_neg.p\",\"rb\"))\n",
    "    return test_positive_reviews, test_negative_reviews\n",
    "\n",
    "def label_data(positive_revs, negative_revs):\n",
    "    # Generate the labels\n",
    "    positive_labels = [[0, 1] for _ in positive_revs]\n",
    "    negative_labels = [[1, 0] for _ in negative_revs]\n",
    "    \n",
    "    # Concatenates the positive and negative labels for train and val\n",
    "    y_labels = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    \n",
    "    x_train = positive_revs + negative_revs\n",
    "     \n",
    "    return [x_train, y_labels]\n",
    "    \n",
    "def __split_train_validation(x_train, y_train, amount_val=.25):\n",
    "    x_train = np.array(x_train)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    # Randomly shuffle data\n",
    "    np.random.seed(10)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y_train)))\n",
    "    print (shuffle_indices)\n",
    "    x_shuffled = x_train[shuffle_indices]\n",
    "    y_shuffled = y_train[shuffle_indices]\n",
    "    \n",
    "    total_reviews = len(x_shuffled)\n",
    "    training_num = total_reviews - int(total_reviews * amount_val)\n",
    "    \n",
    "    x_t = x_shuffled[:training_num]\n",
    "    y_t = y_shuffled[:training_num]\n",
    "    \n",
    "    x_dev = x_shuffled[training_num:]\n",
    "    y_dev = y_shuffled[training_num:]\n",
    "    \n",
    "    return [x_t, y_t], [x_dev, y_dev]\n",
    "\n",
    "def get_train_validation(train_pos, train_neg, amount_val=.25):\n",
    "    # Divides the sets\n",
    "    total_reviews = len(train_pos)\n",
    "    print(\"Num Total Reviews in set:\", total_reviews)\n",
    "    training_num = total_reviews - int(total_reviews * amount_val)\n",
    "    print(\"Num Training Reviews:\", training_num)\n",
    "    \n",
    "    train_pos_reviews_t = train_pos[:training_num]\n",
    "    train_neg_reviews_t = train_neg[:training_num]\n",
    "    train_pos_reviews_v = train_pos[training_num:]\n",
    "    train_neg_reviews_v = train_neg[training_num:]\n",
    "    \n",
    "    # Generate the labels\n",
    "    train_positive_labels = [[0, 1] for _ in train_pos_reviews_t]\n",
    "    val_positive_labels = [[0, 1] for _ in train_pos_reviews_v]\n",
    "    \n",
    "    train_negative_labels = [[1, 0] for _ in train_neg_reviews_t]\n",
    "    val_negative_labels = [[1, 0] for _ in train_neg_reviews_v]\n",
    "    \n",
    "    # Concatenates the positive and negative labels for train and val\n",
    "    y_train = np.concatenate([train_positive_labels, train_negative_labels], 0)\n",
    "    y_val = np.concatenate([val_positive_labels, val_negative_labels], 0)\n",
    "    \n",
    "    # Creates one list for positive and negative reviews\n",
    "    x_train = train_pos_reviews_t + train_neg_reviews_t\n",
    "    x_val = train_pos_reviews_v + train_neg_reviews_v\n",
    "    \n",
    "    print(\"x_train:\", len(x_train))\n",
    "    print(\"y_train:\", len(y_train))\n",
    "    print(\"x_val:\", len(x_val))\n",
    "    print(\"y_val:\", len(y_val))\n",
    "    \n",
    "    return [x_train, y_train],[x_val, y_val]\n",
    "\n",
    "def get_test_labeled(test_pos, test_neg):\n",
    "    # Generate the labels\n",
    "    test_positive_labels = [[0, 1] for _ in test_pos]\n",
    "    test_negative_labels = [[1, 0] for _ in test_neg]\n",
    "    \n",
    "    y = np.concatenate([test_positive_labels, test_negative_labels], 0)\n",
    "    x_test = test_pos + test_neg\n",
    "    \n",
    "    return [x_test, y]\n",
    "    \n",
    "#train, validation = get_train_validation(train_positive_reviews, train_negative_reviews)\n",
    "x_t, y_t = label_data(train_positive_reviews, train_negative_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18634  1333 20315 ..., 17728  7293 17673]\n"
     ]
    }
   ],
   "source": [
    "# Label the data\n",
    "x_train, y_train = label_data(train_positive_reviews, train_negative_reviews)\n",
    "# Separates in Train and Dev\n",
    "x_train_list, x_dev_list = split_train_validation(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loads the vocabulary\n",
    "def load_vocabulary(file_path, num_words=10000):\n",
    "    with open(file_path) as vocab:\n",
    "        vocab_list = [next(vocab) for x in range(num_words)]\n",
    "    vocab_list = [str(vocab).strip() for vocab in vocab_list]\n",
    "    return vocab_list\n",
    "#\n",
    "#load_vocabulary(\"data/vocab_unigrams_no_counts/part-00000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Spark Unigrams\n",
    "text_file = sc.textFile('all_train.txt')\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")).map(lambda word:(word, 1)).reduceByKey(lambda a, b: a+b).sortBy(lambda a: -a[1])\n",
    "# Comment this line, if you want tuples\n",
    "just_words = counts.map(lambda tuple: tuple[0])\n",
    "just_words.saveAsTextFile(\"vocab_unigrams_no_counts\")\n",
    "\n",
    "# Spark Bi-grams\n",
    "bigrams = text_file.map(lambda x:x.split()).flatMap(lambda x: [((x[i],x[i+1]),1) for i in range(0,len(x)-1)])\n",
    "count_bigrams = bigrams.reduceByKey(lambda x, y: x+y).sortBy(lambda a: -a[1])\n",
    "just_bigrams = count_bigrams.map(lambda tuple: tuple[0][0] + ' ' + tuple[0][1])\n",
    "just_bigrams.saveAsTextFile(\"vocab_bigrams_no_counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "# This is a test for the vocabulary\n",
    "\n",
    "vocabulary = load_vocabulary(\"data/vocab_unigrams_no_counts/part-00000\")\n",
    "vocabulary = [str(vocab).strip() for vocab in vocabulary]\n",
    "vocabulary[:5]\n",
    "max_len_vocabulary = len(vocabulary)\n",
    "print (max_len_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "train_reviews = train_positive_reviews + train_negative_reviews\n",
    "print(len(train_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def set_oov(reviews, vocabulary):\n",
    "    updated_reviews = []\n",
    "    for review in reviews:\n",
    "        up_review = []\n",
    "        splitted_review = review.split(\" \")\n",
    "        for i, word in enumerate(splitted_review):\n",
    "            if word not in vocabulary:\n",
    "                splitted_review[i] = 'oov'\n",
    "            else:\n",
    "                splitted_review[i] = word\n",
    "        new_review = (' ').join(splitted_review)\n",
    "        updated_reviews.append(new_review)\n",
    "    return updated_reviews\n",
    "            \n",
    "def set_oov_tag(reviews, vocabulary):\n",
    "    updated_reviews = []\n",
    "    set_vocabulary = set(vocabulary)\n",
    "    for review in reviews:\n",
    "        set_review = set(review.split(\" \"))\n",
    "        oov_words = set_review - set_vocabulary\n",
    "        #print(list(oov_words))\n",
    "        \n",
    "        dic_oov_words = {k:'oov' for k in oov_words}\n",
    "        #print(dic_oov_words)\n",
    "        if len(dic_oov_words) >= 1:\n",
    "            rep = dict((re.escape(k), v) for k, v in dic_oov_words.items())\n",
    "            pattern = re.compile(\"|\".join(rep.keys()))\n",
    "            oov_review = pattern.sub(lambda m: rep[re.escape(m.group(0))], review)\n",
    "            updated_reviews.append(oov_review)\n",
    "        else:\n",
    "            updated_reviews.append(review)\n",
    "    return updated_reviews\n",
    "\n",
    "oov_reviews = set_oov(train_reviews, vocabulary)\n",
    "#print(len(new_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(oov_reviews))\n",
    "super_review = ' '.join(oov_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10001\n"
     ]
    }
   ],
   "source": [
    "print(len(set(super_review.split(\" \"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(oov_reviews, open( \"data/reviews_oov.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCE = 200\n",
    "\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length=MAX_SENTENCE, \n",
    "                                                          vocabulary=vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'add'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-f0edb535e0d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_processor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_reviews\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/bulos87/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, unused_y)\u001b[0m\n\u001b[0;32m    166\u001b[0m       \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_document_length\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mWord\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mid\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m     \"\"\"\n\u001b[1;32m--> 168\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bulos87/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, raw_documents, unused_y)\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_frequency\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_frequency\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'add'"
     ]
    }
   ],
   "source": [
    "list(vocab_processor.fit_transform(train_reviews[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1   2   3   4   5   6   7   8   9  10  11  12  13   2  14  15  16  17\n",
      "  18  19  20   6  21  22  10  23  24  25   1  26  27  28  29  30  21  31\n",
      "  32  30  33  34  25   1  35  36  37  16  38  39  40  41  20   6  42  43\n",
      "   3  44  45  21  46   1   2  47  39  48   3  49  11  50  10  23   2  51\n",
      "  30  52  30  53  54  10  55  56  23  18  57  58  14  59  18  60  18  61\n",
      "  62  14  63  64  21  65  66  10  57  25  13  67  68  27  69  70  64  71\n",
      "  72  21  73  61  74  75  76  77  21  78  79  18  80  30  81  82  83  84\n",
      "  71  85  86  87  88  89  90  18  25   2  21  91  18  92  93   6  21  94\n",
      "  95  96  97  98   6  99  39  69 100  96 101 102 103   6 104  18 105  71\n",
      " 106 107 108  18 109  25  96 101  87 110 111   6 108  43  61 112 113 114\n",
      " 115 116  21  94   2 117 118  18  91 119 120 102 121 122 123 124  18  21\n",
      "  31  61]\n"
     ]
    }
   ],
   "source": [
    "print(x_vocab[0])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
