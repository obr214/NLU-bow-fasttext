{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    string = re.sub(r\"<br />\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "def get_reviews(path, clean = True):\n",
    "    complete_path = path + '/*.txt'\n",
    "    files = glob.glob(complete_path)    \n",
    "    reviews = [str(open(rev).readlines()[0]).strip() for rev in files]\n",
    "    if clean:\n",
    "        reviews = [clean_str(rev) for rev in reviews]\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Gets all the reviews\n",
    "train_positive_reviews = get_reviews(\"data/aclImdb/train/pos\")\n",
    "train_negative_reviews = get_reviews(\"data/aclImdb/train/neg\")\n",
    "test_positive_reviews = get_reviews(\"data/aclImdb/test/pos\")\n",
    "test_negative_reviews = get_reviews(\"data/aclImdb/test/neg\")\n",
    "\n",
    "# Divide The train set into train and validation\n",
    "\n",
    "# Concat all train reviews and write it on a file\n",
    "train_reviews = train_positive_reviews + train_negative_reviews\n",
    "output_train = open('data/all_train.txt', 'w')\n",
    "for rev in train_reviews:\n",
    "    print>>output_train, rev\n",
    "output_train.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saves the Train/Test lists into pickle objects\n",
    "pickle.dump(train_positive_reviews, open( \"data/train_pos.p\", \"wb\" ))\n",
    "pickle.dump(train_negative_reviews, open( \"data/train_neg.p\", \"wb\" ))\n",
    "pickle.dump(test_positive_reviews, open( \"data/test_pos.p\", \"wb\" ))\n",
    "pickle.dump(test_negative_reviews, open( \"data/test_neg.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loads the Train/Test objects\n",
    "train_positive_reviews = pickle.load(open(\"data/train_pos.p\",\"rb\"))\n",
    "train_negative_reviews = pickle.load(open(\"data/train_neg.p\",\"rb\"))\n",
    "test_positive_reviews = pickle.load(open(\"data/test_pos.p\",\"rb\"))\n",
    "test_negative_reviews = pickle.load(open(\"data/test_neg.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_train_sets():\n",
    "    train_positive_reviews = pickle.load(open(\"data/train_pos.p\",\"rb\"))\n",
    "    train_negative_reviews = pickle.load(open(\"data/train_neg.p\",\"rb\"))\n",
    "    return train_positive_reviews, train_negative_reviews\n",
    "\n",
    "def get_test_sets():\n",
    "    test_positive_reviews = pickle.load(open(\"data/test_pos.p\",\"rb\"))\n",
    "    test_negative_reviews = pickle.load(open(\"data/test_neg.p\",\"rb\"))\n",
    "    return test_positive_reviews, test_negative_reviews\n",
    "\n",
    "def label_data(positive_revs, negative_revs):\n",
    "    # Generate the labels\n",
    "    positive_labels = [[0, 1] for _ in positive_revs]\n",
    "    negative_labels = [[1, 0] for _ in negative_revs]\n",
    "    \n",
    "    # Concatenates the positive and negative labels for train and val\n",
    "    y_labels = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    \n",
    "    x_train = positive_revs + negative_revs\n",
    "     \n",
    "    return [x_train, y_labels]\n",
    "    \n",
    "def __split_train_validation(x_train, y_train, amount_val=.25):\n",
    "    x_train = np.array(x_train)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    # Randomly shuffle data\n",
    "    np.random.seed(10)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y_train)))\n",
    "    print (shuffle_indices)\n",
    "    x_shuffled = x_train[shuffle_indices]\n",
    "    y_shuffled = y_train[shuffle_indices]\n",
    "    \n",
    "    total_reviews = len(x_shuffled)\n",
    "    training_num = total_reviews - int(total_reviews * amount_val)\n",
    "    \n",
    "    x_t = x_shuffled[:training_num]\n",
    "    y_t = y_shuffled[:training_num]\n",
    "    \n",
    "    x_dev = x_shuffled[training_num:]\n",
    "    y_dev = y_shuffled[training_num:]\n",
    "    \n",
    "    return [x_t, y_t], [x_dev, y_dev]\n",
    "\n",
    "def get_train_validation(train_pos, train_neg, amount_val=.25):\n",
    "    # Divides the sets\n",
    "    total_reviews = len(train_pos)\n",
    "    print(\"Num Total Reviews in set:\", total_reviews)\n",
    "    training_num = total_reviews - int(total_reviews * amount_val)\n",
    "    print(\"Num Training Reviews:\", training_num)\n",
    "    \n",
    "    train_pos_reviews_t = train_pos[:training_num]\n",
    "    train_neg_reviews_t = train_neg[:training_num]\n",
    "    train_pos_reviews_v = train_pos[training_num:]\n",
    "    train_neg_reviews_v = train_neg[training_num:]\n",
    "    \n",
    "    # Generate the labels\n",
    "    train_positive_labels = [[0, 1] for _ in train_pos_reviews_t]\n",
    "    val_positive_labels = [[0, 1] for _ in train_pos_reviews_v]\n",
    "    \n",
    "    train_negative_labels = [[1, 0] for _ in train_neg_reviews_t]\n",
    "    val_negative_labels = [[1, 0] for _ in train_neg_reviews_v]\n",
    "    \n",
    "    # Concatenates the positive and negative labels for train and val\n",
    "    y_train = np.concatenate([train_positive_labels, train_negative_labels], 0)\n",
    "    y_val = np.concatenate([val_positive_labels, val_negative_labels], 0)\n",
    "    \n",
    "    # Creates one list for positive and negative reviews\n",
    "    x_train = train_pos_reviews_t + train_neg_reviews_t\n",
    "    x_val = train_pos_reviews_v + train_neg_reviews_v\n",
    "    \n",
    "    print(\"x_train:\", len(x_train))\n",
    "    print(\"y_train:\", len(y_train))\n",
    "    print(\"x_val:\", len(x_val))\n",
    "    print(\"y_val:\", len(y_val))\n",
    "    \n",
    "    return [x_train, y_train],[x_val, y_val]\n",
    "\n",
    "def get_test_labeled(test_pos, test_neg):\n",
    "    # Generate the labels\n",
    "    test_positive_labels = [[0, 1] for _ in test_pos]\n",
    "    test_negative_labels = [[1, 0] for _ in test_neg]\n",
    "    \n",
    "    y = np.concatenate([test_positive_labels, test_negative_labels], 0)\n",
    "    x_test = test_pos + test_neg\n",
    "    \n",
    "    return [x_test, y]\n",
    "    \n",
    "#train, validation = get_train_validation(train_positive_reviews, train_negative_reviews)\n",
    "x_t, y_t = label_data(train_positive_reviews, train_negative_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18634  1333 20315 ..., 17728  7293 17673]\n"
     ]
    }
   ],
   "source": [
    "# Label the data\n",
    "x_train, y_train = label_data(train_positive_reviews, train_negative_reviews)\n",
    "# Separates in Train and Dev\n",
    "x_train_list, x_dev_list = split_train_validation(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750\n",
      "18750\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the data\n",
    "def split_train_validation(x_train, y_train, amount_val=.25):\n",
    "    x_train_shuffled = []\n",
    "    y_train_shuffled = []\n",
    "    np.random.seed(10)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y_train)))\n",
    "    for i in shuffle_indices:\n",
    "        x_train_shuffled.append(x_train[i])\n",
    "        y_train_shuffled.append(y_train[i])\n",
    "    \n",
    "    total_reviews = len(x_train_shuffled)\n",
    "    training_num = total_reviews - int(total_reviews * amount_val)\n",
    "\n",
    "    x_t = x_train_shuffled[:training_num]\n",
    "    y_t = y_train_shuffled[:training_num]\n",
    "\n",
    "    x_dev = x_train_shuffled[training_num:]\n",
    "    y_dev = y_train_shuffled[training_num:]\n",
    "\n",
    "    return [x_t, y_t], [x_dev, y_dev]\n",
    "\n",
    "# Separates in Train and Dev\n",
    "x_train_list, x_dev_list = split_train_validation(x_t, y_t)\n",
    "print(len(x_train_list[0]))\n",
    "print(len(x_train_list[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "another of many nearly forgotten movies cranked out by poverty row in the 1930 's , resurrected by the magic of dvd starring stock universal player lionel atwill \\( often a supporting actor in numerous frankenstein movies \\) as a pair of twins involved in a murder racket one kills the victims \\( stockbrokers involved in a scam \\) and asks witnesses for the exact time , while the other is deaf and is proved innocent because he could not have spoken to witnesses of course , where it falls apart is if it was a congenital deafness , would n't they both be deaf \\? oh , well atwill does a pretty good job here , faking being deaf and mute unfortunately , no one else here can really act worth a darn\n",
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "print(x_dev_list[0][1])\n",
    "print(x_dev_list[1][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loads the vocabulary\n",
    "def load_vocabulary(file_path, num_words=10000):\n",
    "    with open(file_path) as vocab:\n",
    "        vocab_list = [next(vocab) for x in range(num_words)]\n",
    "    vocab_list = [str(vocab).strip() for vocab in vocab_list]\n",
    "    return vocab_list\n",
    "#\n",
    "#load_vocabulary(\"data/vocab_unigrams_no_counts/part-00000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Spark Unigrams\n",
    "text_file = sc.textFile('all_train.txt')\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")).map(lambda word:(word, 1)).reduceByKey(lambda a, b: a+b).sortBy(lambda a: -a[1])\n",
    "# Comment this line, if you want tuples\n",
    "just_words = counts.map(lambda tuple: tuple[0])\n",
    "just_words.saveAsTextFile(\"vocab_unigrams_no_counts\")\n",
    "\n",
    "# Spark Bi-grams\n",
    "bigrams = text_file.map(lambda x:x.split()).flatMap(lambda x: [((x[i],x[i+1]),1) for i in range(0,len(x)-1)])\n",
    "count_bigrams = bigrams.reduceByKey(lambda x, y: x+y).sortBy(lambda a: -a[1])\n",
    "just_bigrams = count_bigrams.map(lambda tuple: tuple[0][0] + ' ' + tuple[0][1])\n",
    "just_bigrams.saveAsTextFile(\"vocab_bigrams_no_counts\")\n",
    "just_bigrams.saveAsTextFile(\"vocab_oov_bigrams_no_counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "# This is a test for the vocabulary\n",
    "\n",
    "vocabulary = load_vocabulary(\"data/vocab_unigrams_no_counts/part-00000\")\n",
    "vocabulary = [str(vocab).strip() for vocab in vocabulary]\n",
    "vocabulary[:5]\n",
    "max_len_vocabulary = len(vocabulary)\n",
    "print (max_len_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "train_reviews = train_positive_reviews + train_negative_reviews\n",
    "print(len(train_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def set_oov(reviews, vocabulary):\n",
    "    updated_reviews = []\n",
    "    for review in reviews:\n",
    "        up_review = []\n",
    "        splitted_review = review.split(\" \")\n",
    "        for i, word in enumerate(splitted_review):\n",
    "            if word not in vocabulary:\n",
    "                splitted_review[i] = 'oov'\n",
    "            else:\n",
    "                splitted_review[i] = word\n",
    "        new_review = (' ').join(splitted_review)\n",
    "        updated_reviews.append(new_review)\n",
    "    return updated_reviews\n",
    "            \n",
    "def set_oov_tag(reviews, vocabulary):\n",
    "    updated_reviews = []\n",
    "    set_vocabulary = set(vocabulary)\n",
    "    for review in reviews:\n",
    "        set_review = set(review.split(\" \"))\n",
    "        oov_words = set_review - set_vocabulary\n",
    "        #print(list(oov_words))\n",
    "        \n",
    "        dic_oov_words = {k:'oov' for k in oov_words}\n",
    "        #print(dic_oov_words)\n",
    "        if len(dic_oov_words) >= 1:\n",
    "            rep = dict((re.escape(k), v) for k, v in dic_oov_words.items())\n",
    "            pattern = re.compile(\"|\".join(rep.keys()))\n",
    "            oov_review = pattern.sub(lambda m: rep[re.escape(m.group(0))], review)\n",
    "            updated_reviews.append(oov_review)\n",
    "        else:\n",
    "            updated_reviews.append(review)\n",
    "    return updated_reviews\n",
    "\n",
    "oov_reviews = set_oov(train_reviews, vocabulary)\n",
    "#print(len(new_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(oov_reviews))\n",
    "super_review = ' '.join(oov_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepares Train/Dev for FaceBook FastText\n",
    "# Loads the Data\n",
    "train_positive_reviews = pickle.load(open(\"data/train_pos.p\",\"rb\"))\n",
    "train_negative_reviews = pickle.load(open(\"data/train_neg.p\",\"rb\"))\n",
    "\n",
    "# For each review append the label\n",
    "train_pos_reviews_labeled = [x + ' __label__1' for x in train_positive_reviews]\n",
    "train_neg_reviews_labeled = [x + ' __label__0' for x in train_negative_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is one of the most daring and important of the so called pre code films made in hollywood during the 1930s unlike some pre code films that occasionally dabbled in subjects that would have never been allowed after 1934 5 , this film fully immersed itself in a very sordid yet entertaining plot from start to finish the conventional morality of the late 30s and 40s was definitely not evident in this film , as the film is essentially about a conniving woman who sleeps her way to the top and with no apologies along the way this broad both enjoyed sex and used it on every man who could help her get rich something you just never would have seen in films made just two or three years later the film begins with barbara stanwyck working in her father 's speakeasy in addition to being her boss , he is also her pimp and encourages her to sleep with a local government official so that he 'll allow the illegal bar to operate with impunity while not especially clear here , it appears as if daddy has been renting his daughter 's body out for a long time however , after nearly being raped and attacking this man by breaking a bottle of beer over his skull , barbara has had enough and heads to the big city it does n't hurt that the still blew up and killed her father , but her feeling that she was whoring herself out and had nothing to show for it appeared to be the impetus to move despite the depression , barbara uses sex to get a job at a huge mega bank she starts out at a pretty menial job as a file clerk , but in the space of what seems like just a few weeks , she sleeps her way from one job to another to yet another until she is sleeping with the head of the bank and his future son in law ! ! ! this all ends in tragedy , but babs does n't seem too shook up over the deaths of these two men in fact , some time later , she is able to insinuate herself into the life of the new ceo and once again she 's on top \\( perhaps in more way than one \\) now so far , this is a wonderful movie because it was so gritty and unrepentant barbara played a 100 sociopath a woman with no morality and no conscience just a desire to squeeze as much out of life as she could no matter who she hurt in the process however , the brave writers and producer chickened out and thought it important to tack on a redemptive ending considering that this woman was so evil and conniving , her change of heart at the end was a major disappointment and strongly detracted from the film in many ways , this reminded me of the ending of jezebel as once again , a wicked person somehow sees the light and changes at the not too convincing conclusion my advice is to try watching red headed woman and downstairs red headed woman is much like baby face but features no magical transformation at the end the leading lady really is a skunk down deep ! in downstairs , a film very much like red headed woman , the roles are reversed and a man \\( john gilbert \\) plays a similar conniving character both are classics and a bit better than this film this film is an amazing curio of a brief period of often ultra sleazy hollywood films and in this light it 's well worth seeing for cinephiles also , fans of the duke take note john wayne plays a very small part in the film and it 's very unusual to see a very young wayne playing such a conventional role __label__1\n"
     ]
    }
   ],
   "source": [
    "fb_reviews = train_pos_reviews_labeled + train_neg_reviews_labeled\n",
    "\n",
    "shuffle(fb_reviews)\n",
    "print(fb_reviews[0])\n",
    "\n",
    "with open('fastText/fb_train_shuffled.txt', mode='wt', encoding='utf-8') as output_fb_train:\n",
    "    output_fb_train.write('\\n'.join(fb_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepares Test for Facebook FastText\n",
    "test_positive_reviews = pickle.load(open(\"data/test_pos.p\",\"rb\"))\n",
    "test_negative_reviews = pickle.load(open(\"data/test_neg.p\",\"rb\"))\n",
    "\n",
    "# For each review append the label\n",
    "test_pos_reviews_labeled = [x + ' __label__1' for x in test_positive_reviews]\n",
    "test_neg_reviews_labeled = [x + ' __label__0' for x in test_negative_reviews]\n",
    "\n",
    "fb_test_reviews = test_pos_reviews_labeled + test_neg_reviews_labeled\n",
    "\n",
    "shuffle(fb_test_reviews)\n",
    "\n",
    "with open('fastText/fb_test_shuffled.txt', mode='wt', encoding='utf-8') as output_fb_test:\n",
    "    output_fb_test.write('\\n'.join(fb_test_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Such a shame that this wonderful bright spot on the small screen had such talent in writers and actors, such wonderful scenery that was the ultimate escapism for those in a land locked, sun deprived state. Many of the actors went on to bigger things...another indicator that there was something wonderful sadly lost. I lived in Columbus, Ohio at the time, with all of my (now ex) husband's very large family in a chorus of 'NO NO!!' every time it would be yet again taken over by a baseball game broadcast. <br /><br />Someone who wrote here mentioned it was up against 'Rosanne'...all my ex and I noticed was that it was always always always preempted by BASEBALL!!!! Yes, FOX really wasted something wonderful that one and nothing will ever equal it~! Thank you for the memories of it. __label__1\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_positive_reviews = get_reviews(\"data/aclImdb/train/pos\", clean=False)\n",
    "train_negative_reviews = get_reviews(\"data/aclImdb/train/neg\", clean=False)\n",
    "test_positive_reviews = get_reviews(\"data/aclImdb/test/pos\", clean=False)\n",
    "test_negative_reviews = get_reviews(\"data/aclImdb/test/neg\", clean=False)\n",
    "\n",
    "\n",
    "from random import shuffle\n",
    "# For each review append the label\n",
    "train_pos_reviews_labeled = [x + ' __label__1' for x in train_positive_reviews]\n",
    "train_neg_reviews_labeled = [x + ' __label__0' for x in train_negative_reviews]\n",
    "\n",
    "fb_reviews = train_pos_reviews_labeled + train_neg_reviews_labeled\n",
    "\n",
    "shuffle(fb_reviews)\n",
    "\n",
    "with open('fastText/fb_train_unclean_shuffled.txt', mode='wt', encoding='utf-8') as output_fb_train:\n",
    "    output_fb_train.write('\\n'.join(fb_reviews))\n",
    "    \n",
    "#=============================\n",
    "\n",
    "# For each review append the label\n",
    "test_pos_reviews_labeled = [x + ' __label__1' for x in test_positive_reviews]\n",
    "test_neg_reviews_labeled = [x + ' __label__0' for x in test_negative_reviews]\n",
    "\n",
    "fb_test_reviews = test_pos_reviews_labeled + test_neg_reviews_labeled\n",
    "\n",
    "shuffle(fb_test_reviews)\n",
    "\n",
    "with open('fastText/fb_test_unclean_shuffled.txt', mode='wt', encoding='utf-8') as output_fb_test:\n",
    "    output_fb_test.write('\\n'.join(fb_test_reviews))\n",
    "    \n",
    "fb_test_reviews[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BIGRAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train_reviews_oov = pickle.load(open(\"data/reviews_oov.p\", \"rb\"))\n",
    "# Set this to file\n",
    "with open('data/reviews_oov.txt', mode='wt', encoding='utf-8') as output_reviews_oov:\n",
    "    output_reviews_oov.write('\\n'.join(x_train_reviews_oov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"oov_is is_a a_young oov oov of_oov oov oov oov oov oov oov_, ,_he he_is is_very oov oov oov oov oov oov out_of of_the the_blue oov ,_his his_father father_oov oov_that that_oov oov_will will_be oov oov him_to to_the the_oov oov_\\\\( \\\\(_oov oov_\\\\) \\\\)_to to_oov oov_something something_that that_oov oov_has has_no oov interest_in oov oov oov oov oov out_of of_oov oov_as as_a a_result result_, ,_from from_the the_start start_, ,_oov oov_is oov oov oov being_a oov oov oov man_, ,_his his_father father_is is_difficult difficult_to to_talk talk_to oov oov oov his_oov oov_both oov father_and and_son oov oov oov oov and_oov oov_and and_it it_'s 's_very oov oov when_the oov oov oov his_son oov that_he he_should should_not not_be be_so oov oov when_i i_read read_the oov oov ,_it oov oov about_how how_much much_the the_characters oov oov oov began_to to_know oov each_other oov however_, ,_i i_really really_do do_n't n't_think think_they they_did oov and_that that_is is_the oov oov oov oov aspect_of of_the the_film oov sure_, ,_there there_were oov oov oov oov ,_but but_so so_often oov there_was was_an an_oov oov_of of_oov oov_and and_oov oov_i i_actually oov liked_this this_and oov oov that_there there_was was_n't oov oov oov of_this this_as as_it it_would would_have oov oov oov overall_, ,_the the_film film_is is_well well_acted acted_and oov oov oov oov_an oov oov insight_into oov oov and_the the_oov oov_it it_also oov provides_a a_fascinating oov oov_of oov oov oov and_the the_oov oov oov oov while_the the_slow oov pace_and oov lack_of oov oov about_the the_relationship oov throughout_the the_film oov oov oov oov ,_i i_think think_it oov gave_the the_film oov oov oov and_made made_it it_look look_like like_a a_film film_about oov oov oov oov oov a_nice nice_and oov oov\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_test = x_train_reviews_oov[0]\n",
    "\n",
    "# Loads vocab\n",
    "bi_vocabulary = load_vocabulary(\"data/vocab_oov_bigrams_no_counts/part-00000\")\n",
    "\n",
    "def find_bigrams(review, vocabulary):\n",
    "    split_review = review.split(' ')\n",
    "    zipped = zip(split_review, split_review[1:])\n",
    "    bigrams = [x[0] + '_' + x[1] if x[0] + ' ' + x[1] in vocabulary else 'oov' for x in zipped]\n",
    "    print(len(bigrams))\n",
    "    return ' '.join(bigrams)\n",
    "\n",
    "#[find_bigrams(rev, bi_vocabulary) for rev in x_train_reviews_oov]\n",
    "\n",
    "find_bigrams(rev_test, bi_vocabulary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([2, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([3, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([4, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([5, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([6, 0, 0, 0, 0, 0, 0, 0, 0, 0])]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vp = learn.preprocessing.VocabularyProcessor(10)\n",
    "list(vp.fit_transform([\"a\", \"dog\" , \"ran\" ,\"in\" ,\"the\", \"park\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 'a', 'b', 'c'], [4, 5, 6, 'd', 'e', 'f']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [[1,2,3], [4,5,6]]\n",
    "y = [['a', 'b', 'c'], ['d','e','f']]\n",
    "\n",
    "zipped = zip(x,y)\n",
    "final_revs = [x[0]+x[1] for x in zipped]\n",
    "final_revs"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
