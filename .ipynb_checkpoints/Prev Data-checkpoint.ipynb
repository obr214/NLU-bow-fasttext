{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "def get_reviews(path, clean = True):\n",
    "    complete_path = path + '/*.txt'\n",
    "    files = glob.glob(complete_path)    \n",
    "    reviews = [str(open(rev).readlines()[0]).strip() for rev in files]\n",
    "    # Removes the tag <br />\n",
    "    reviews = [rev.replace('<br />',' ') for rev in reviews]\n",
    "    if clean:\n",
    "        reviews = [clean_str(rev) for rev in reviews]\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Gets all the reviews\n",
    "train_positive_reviews = get_reviews(\"data/aclImdb/train/pos\")\n",
    "train_negative_reviews = get_reviews(\"data/aclImdb/train/neg\")\n",
    "test_positive_reviews = get_reviews(\"data/aclImdb/test/pos\")\n",
    "test_negative_reviews = get_reviews(\"data/aclImdb/test/neg\")\n",
    "\n",
    "# Divide The train set into train and validation\n",
    "\n",
    "# Concat all train reviews and write it on a file\n",
    "train_reviews = train_positive_reviews + train_negative_reviews\n",
    "output_train = open('data/all_train.txt', 'w')\n",
    "for rev in train_reviews:\n",
    "    print>>output_train, rev\n",
    "output_train.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saves the Train/Test lists into pickle objects\n",
    "pickle.dump(train_positive_reviews, open( \"data/train_pos.p\", \"wb\" ))\n",
    "pickle.dump(train_negative_reviews, open( \"data/train_neg.p\", \"wb\" ))\n",
    "pickle.dump(test_positive_reviews, open( \"data/test_pos.p\", \"wb\" ))\n",
    "pickle.dump(test_negative_reviews, open( \"data/test_neg.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loads the Train/Test objects\n",
    "train_positive_reviews = pickle.load(open(\"data/train_pos.p\",\"rb\"))\n",
    "train_negative_reviews = pickle.load(open(\"data/train_neg.p\",\"rb\"))\n",
    "test_positive_reviews = pickle.load(open(\"data/test_pos.p\",\"rb\"))\n",
    "test_negative_reviews = pickle.load(open(\"data/test_neg.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_labes(reviews_list, pos=True):\n",
    "    # Generate labels\n",
    "    positive_labels = [[0, 1] for _ in train_pos]\n",
    "    negative_labels = [[1, 0] for _ in train_neg]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    return [x_text, y]\n",
    "train_data = add_labes(train_positive_reviews, train_negative_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " ..., \n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reviews: 12500\n",
      "9375\n",
      "Training 18750\n",
      "Validation 6250\n"
     ]
    }
   ],
   "source": [
    "def divide_train(train_pos, train_neg, amount_val=.25):\n",
    "    total_reviews = len(train_pos)\n",
    "    training_num = total_reviews - int(total_reviews * amount_val)\n",
    "    \n",
    "    train_pos_reviews_t = train_pos[:training_num]\n",
    "    train_neg_reviews_t = train_neg[:training_num]\n",
    "    train_pos_reviews_v = train_pos[training_num:]\n",
    "    train_neg_reviews_v = train_neg[training_num:]\n",
    "    \n",
    "    train_reviews_t = train_pos_reviews_t + train_neg_reviews_t\n",
    "    train_reviews_v = train_pos_reviews_v + train_neg_reviews_v\n",
    "    \n",
    "    return train_reviews_t, train_reviews_v\n",
    "\n",
    "divide_train(train_positive_reviews, train_negative_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "the\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loads the vocabulary\n",
    "def load_vocabulary(file_path, num_words=10000):\n",
    "    with open(file_path) as vocab:\n",
    "        vocab_list = [next(vocab) for x in xrange(num_words)]\n",
    "    return vocab_list\n",
    "\n",
    "load_vocabulary(\"data/vocab_unigrams_no_counts/part-00000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Spark Unigrams\n",
    "text_file = sc.textFile('all_train.txt')\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")).map(lambda word:(word, 1)).reduceByKey(lambda a, b: a+b).sortBy(lambda a: -a[1])\n",
    "# Comment this line, if you want tuples\n",
    "just_words = counts.map(lambda tuple: tuple[0])\n",
    "just_words.saveAsTextFile(\"vocab_unigrams_no_counts\")\n",
    "\n",
    "# Spark Bi-grams\n",
    "bigrams = text_file.map(lambda x:x.split()).flatMap(lambda x: [((x[i],x[i+1]),1) for i in range(0,len(x)-1)])\n",
    "count_bigrams = bigrams.reduceByKey(lambda x, y: x+y).sortBy(lambda a: -a[1])\n",
    "just_bigrams = count_bigrams.map(lambda tuple: tuple[0][0] + ' ' + tuple[0][1])\n",
    "just_bigrams.saveAsTextFile(\"vocab_bigrams_no_counts\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
